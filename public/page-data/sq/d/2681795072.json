{
  "data": {
    "featured": {
      "edges": [
        {
          "node": {
            "frontmatter": {
              "title": "Voice Conversion Transformer",
              "cover": {
                "childImageSharp": {
                  "gatsbyImageData": {
                    "layout": "constrained",
                    "placeholder": {
                      "fallback": "data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAALCAYAAAB/Ca1DAAAACXBIWXMAABYlAAAWJQFJUiTwAAACBElEQVQoz5WSS2/TQBSF/eNhxYYdayqBUBFSd62gqDxEQTShQoQ0JCl17bRx/PaM3zP2hzwpRZVgwZGOZuZqdB/nXIu/oO+37Ax7+r6n6/ptzJydOQcOEEIQRRFKKawoipEiY+EK9j9dc3kVQ99tM94W+HMXIqMoyjsNJEmC67oEQYDVtMpU/H6R8uLtCtcThHHK0gk5OLY5nceUtWI8izn87DC/8GiampOzkONJSNP2SCnZeGuiKMQasudScGZnvB55pLJiuljx/ovN7tEVh2MfWSrT/VBwNHEIAp+9dw7PDm0SUbLxrjmd/mS19rGqqkZrxWgW8fzIwV2ngDZjt5WgkDFSitvxNhuP+WJB4LkEnkOeSxPXN6pY/zQFKKvasG1btO7RXW/Ev3IcHPsSPwjxPM9o5/sbkjjGqusapVpa1ZGXiqZp+R+cL5dMJhPDxXKJNVQpC8mHbwH3d6aMp9fotiTNcvwoQ6nOuKx0T5RINkFiGmhaTdN2iDSlaRqzMsM/SymN1ppENribAt3B2bnHm5HNw90ZOwc2ad7y5KXNg6dTPn51qQrJzv45j/aWrP0UIVLjtNEwDGNykfHqZM29x1NmFwFpkuCHCSKvidOcqm4oKkVRaW522bzLpsN1V8znP8iy7K4pg+CDjnpo8QZSZkRhYBZ3a5Ox7M6iD9MN4/7GL0IpQymk4p+yAAAAAElFTkSuQmCC"
                    },
                    "images": {
                      "fallback": {
                        "src": "/static/877a5e0a1a87fbf7a49a7e7886b5d9b8/e58c6/vc_demo.png",
                        "srcSet": "/static/877a5e0a1a87fbf7a49a7e7886b5d9b8/dfbac/vc_demo.png 175w,\n/static/877a5e0a1a87fbf7a49a7e7886b5d9b8/1a264/vc_demo.png 350w,\n/static/877a5e0a1a87fbf7a49a7e7886b5d9b8/e58c6/vc_demo.png 700w,\n/static/877a5e0a1a87fbf7a49a7e7886b5d9b8/dedab/vc_demo.png 1400w",
                        "sizes": "(min-width: 700px) 700px, 100vw"
                      },
                      "sources": [
                        {
                          "srcSet": "/static/877a5e0a1a87fbf7a49a7e7886b5d9b8/55e84/vc_demo.avif 175w,\n/static/877a5e0a1a87fbf7a49a7e7886b5d9b8/1b102/vc_demo.avif 350w,\n/static/877a5e0a1a87fbf7a49a7e7886b5d9b8/fe1b9/vc_demo.avif 700w,\n/static/877a5e0a1a87fbf7a49a7e7886b5d9b8/d3a2e/vc_demo.avif 1400w",
                          "type": "image/avif",
                          "sizes": "(min-width: 700px) 700px, 100vw"
                        },
                        {
                          "srcSet": "/static/877a5e0a1a87fbf7a49a7e7886b5d9b8/1ab83/vc_demo.webp 175w,\n/static/877a5e0a1a87fbf7a49a7e7886b5d9b8/a8c7f/vc_demo.webp 350w,\n/static/877a5e0a1a87fbf7a49a7e7886b5d9b8/8c88b/vc_demo.webp 700w,\n/static/877a5e0a1a87fbf7a49a7e7886b5d9b8/633d7/vc_demo.webp 1400w",
                          "type": "image/webp",
                          "sizes": "(min-width: 700px) 700px, 100vw"
                        }
                      ]
                    },
                    "width": 700,
                    "height": 374
                  }
                }
              },
              "tech": [
                "Voice conversion",
                "Attention",
                "Transformer",
                "PPG, BNF",
                "Speaker embeddings"
              ],
              "github": null,
              "external": null,
              "cta": "https://bit.ly/vc-bhanu"
            },
            "html": "<p>For Any to Any voice conversion transformer the linguistic features and voice identity of an utterance are seperated and used independently to achieve any combination on voice conversion. BNF and Speaker embeddings are inputs and mel-spectrogram is predicted. Speech quality syntesised is very clear with good voice conversion.</p>"
          }
        },
        {
          "node": {
            "frontmatter": {
              "title": "Repaper",
              "cover": null,
              "tech": ["Transformers", "LayoutLMv3", "Huggingface", "Pytorch"],
              "github": "https://github.com/pvbhanuteja/repaper",
              "external": null,
              "cta": null
            },
            "html": "<p>A python package to create an editable PDF form from a handwritten form. I used LayoutLMv3 model trained on a\nQuestion-Answer dataset to identify key-value pairs from the paper and easy-ocr to extract the bounding boxes and text\ninformation.</p>"
          }
        },
        {
          "node": {
            "frontmatter": {
              "title": "MixrNet",
              "cover": {
                "childImageSharp": {
                  "gatsbyImageData": {
                    "layout": "constrained",
                    "placeholder": {
                      "fallback": "data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAANCAYAAACpUE5eAAAACXBIWXMAABYlAAAWJQFJUiTwAAABl0lEQVQ4y5WTT0vcQBiH/USi30J6Ezx47amwlx5FPfgJpHizKtSDUAVFd0UPRUoRRLQge5Olf2hZDWbdZLObZJJMMvOUiVkRm7Trj7wM5J155n3nNzOmtWYYRlorfnRbXLa/8sv99vDffEW+mFQM+q8Yew7044CPJ3OsbU6zXp8jkOEjJ05iEqVRBqZUKbQE2Gf18zzLW7Ns1BcJYkGaZXnu6HibhaUaJ2cN4lSMBgxDwdb2B740L4iUQClFlCSkQOPTLlOvJnn9ZgbbsfP5Jl8KVEUL/YHP21qNyfEJdvb2zfE9bvb990/Or85pXjcJRThahTKJqTf2WVl5x8FhAyklWZYhU8lz/deUp076vk8URViWhW3b+ei6Lo7rICIxOtCEqShNUzBmDDcqzmp4bXRFlAJVsfB9ENDKMlSS4EURqVK0BgNkxZWpBOoH+7h0nNzh0yBgNQzzijcdB99U/xKgkWnb73aRQtDp9bjzPIQQ3Hse2UsrNDKLby0Lr9/npt3m3rbpdDp4vV4lrLrlp++2RPofpvwBXXnk2PPVhtwAAAAASUVORK5CYII="
                    },
                    "images": {
                      "fallback": {
                        "src": "/static/9d592d528ad639abe4c657e70f67467b/f4b24/mixrnet-demo.png",
                        "srcSet": "/static/9d592d528ad639abe4c657e70f67467b/1ac8f/mixrnet-demo.png 175w,\n/static/9d592d528ad639abe4c657e70f67467b/7d021/mixrnet-demo.png 350w,\n/static/9d592d528ad639abe4c657e70f67467b/f4b24/mixrnet-demo.png 700w",
                        "sizes": "(min-width: 700px) 700px, 100vw"
                      },
                      "sources": [
                        {
                          "srcSet": "/static/9d592d528ad639abe4c657e70f67467b/f0c3a/mixrnet-demo.avif 175w,\n/static/9d592d528ad639abe4c657e70f67467b/39b02/mixrnet-demo.avif 350w,\n/static/9d592d528ad639abe4c657e70f67467b/fc965/mixrnet-demo.avif 700w",
                          "type": "image/avif",
                          "sizes": "(min-width: 700px) 700px, 100vw"
                        },
                        {
                          "srcSet": "/static/9d592d528ad639abe4c657e70f67467b/f74b7/mixrnet-demo.webp 175w,\n/static/9d592d528ad639abe4c657e70f67467b/aa3e5/mixrnet-demo.webp 350w,\n/static/9d592d528ad639abe4c657e70f67467b/46e1e/mixrnet-demo.webp 700w",
                          "type": "image/webp",
                          "sizes": "(min-width: 700px) 700px, 100vw"
                        }
                      ]
                    },
                    "width": 700,
                    "height": 443.99999999999994
                  }
                }
              },
              "tech": ["Mixup", "Regularization", "Resnet", "Pytorch"],
              "github": "https://github.com/pvbhanuteja/mixrnet",
              "external": "https://arxiv.org/pdf/2111.11616.pdf",
              "cta": null
            },
            "html": "<p>Using mixup data augmentation technique as regularization and improving the ResNet50 architecture performance on image classification tasks. Achieved an error of 4.87% on CIFAR‑10 data‑set (Top 105 on CIFAR‑10 bench‑marking).</p>"
          }
        },
        {
          "node": {
            "frontmatter": {
              "title": "Image Colorization",
              "cover": {
                "childImageSharp": {
                  "gatsbyImageData": {
                    "layout": "constrained",
                    "placeholder": {
                      "fallback": "data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAICAIAAAB2/0i6AAAACXBIWXMAAAPoAAAD6AG1e1JrAAAB5UlEQVQY0yXOTWvTYAAA4Jw96a0w2KAKRRmi0gxK12HamtmvNG2z1mKDc+1WR8iHXdKa5k2ahiwxNjNNoaONWqgVwcDs2EVB2GE3L/qXhPn8ggdKpVIkSTIMk81mM5kMjuOGYaiqWqmUE8nt0HY/XjvZpd5HUx34qRjGjmnzoq2OymUSjSegdDpNkqRhGLIsa5pm2/ZkMikUCgRReoKmQ6iG1BxJ/ygAT1Dn0uDcnPzE8Wc7JSKZRCH+WuMaRVHNZhMA0O1K9b2XSAKtcDNSmOfqQ6w+qlKnxQO3CRZH3UF9rxlHHkOe502n08Vi4brubDYzDGM8Hi+XSyB1919R2vTX8YdL98uVPPoxmF1x5oVyevnp/I8isYf7Ncj3/f9Vx3Esy7Jt2/d9SZJisc1y9fnbb787w6+6910YncmWp9ifh2d/GVmLIbfzRAwyTRPDsHw+32g0RFFUVZXneRwv5LFc7UWD1e1sEUZLGwf7OVcgvG65d5TN7azliLul6jqk63okEgkGgyzL0jRtWRbHcYFAAIY3ECTWB1vRrdWVO7dEpqgeYnN9V2HQmys3HkbXwpurkOM4AACapgEAgiAoiqJpGsMwrRbfFljv5L7Zu9fiQv3eI/HNA6MPv9Mj3Ov1TjvMC/A/oILNpCHnBowAAAAASUVORK5CYII="
                    },
                    "images": {
                      "fallback": {
                        "src": "/static/66e47449de0c2c70084fc4079c4e5592/6b0f3/img-clr-demo.webp",
                        "srcSet": "/static/66e47449de0c2c70084fc4079c4e5592/9566d/img-clr-demo.webp 175w,\n/static/66e47449de0c2c70084fc4079c4e5592/9c6c0/img-clr-demo.webp 350w,\n/static/66e47449de0c2c70084fc4079c4e5592/6b0f3/img-clr-demo.webp 700w,\n/static/66e47449de0c2c70084fc4079c4e5592/aa16a/img-clr-demo.webp 1400w",
                        "sizes": "(min-width: 700px) 700px, 100vw"
                      },
                      "sources": [
                        {
                          "srcSet": "/static/66e47449de0c2c70084fc4079c4e5592/cdf5b/img-clr-demo.avif 175w,\n/static/66e47449de0c2c70084fc4079c4e5592/e3697/img-clr-demo.avif 350w,\n/static/66e47449de0c2c70084fc4079c4e5592/39ab9/img-clr-demo.avif 700w,\n/static/66e47449de0c2c70084fc4079c4e5592/4722e/img-clr-demo.avif 1400w",
                          "type": "image/avif",
                          "sizes": "(min-width: 700px) 700px, 100vw"
                        }
                      ]
                    },
                    "width": 700,
                    "height": 283
                  }
                }
              },
              "tech": ["Unet", "VGG Feature loss", "Lab space image", "CNN regression", "Unet"],
              "github": "https://github.com/pvbhanuteja/Image-colorization-UNET",
              "external": null,
              "cta": null
            },
            "html": "<p>Image is converted to lab space(2 channel) to reduce the regression by a channel. Model is trained on UNET architecture. Tried with various loss functions (MSE, SSIM, TVLOSS, Pretrained VGG feature loss). Weighted loss gave better performance.</p>"
          }
        },
        {
          "node": {
            "frontmatter": {
              "title": "Semantic Segmentation - Thesis",
              "cover": {
                "childImageSharp": {
                  "gatsbyImageData": {
                    "layout": "constrained",
                    "placeholder": {
                      "fallback": "data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAMCAIAAADtbgqsAAAACXBIWXMAAAsTAAALEwEAmpwYAAAC3ElEQVQozx3LW0xSYQAA4N8LIHhJUpfT1BJvlFSClhYTUVBSvGCW0GR5UgIvKCQqiuf8/3/+wzkHFF25nMt5WTPLrfXYWm+tl3ppa6s210NPtbUuLz21tk6b3/sHdNVlenP37ud/j79LD3/92/wm7fyQHvyWNn9K61+lje/S2hdp5dPf2Ls/XX5SrdNSQ/1PH++2tjRqNMeBXl9VXl3pxfxAcDSytbXz/uPGm5c+Nvjo4OCVJK2+ed0z3O/0U2drtRXV5Y3WRoOhwu10aCvL1EezgKFWV3Iir0pbfLxIrS0/1mg6Y+s0G41n+pztcDnabm9tqCs5UaxCmD44+CBwjLY0t6RQnZmVnpmhBNY2k6H+tK3TODpxdXTCQQ33QWHJ6590urpnA95lAW+vL7VZTc9fPJckSeSISgbyco4olQqlUg46e61m23m32+4fcU75KQ91vauzw26zzIUmBDR3Lxbe34w/2U4k8FTQe0NTehKkJssUCrkiTSaTgzGfa9Blv0UNeKhBn2fQc/NGcNx7NxENBUZFPLu2RIt4du9+bBGGZwI3h9z2S+d0BTlHc9TpObkqEOUzmOnChXmGgRgyAk0TiAXExhlIWIQxEhBELIKEm8fBer5ewVuy6OzUiDs5gpMBx8khUmGsZ9kqlm1nsVdA3WKkGIcoyPGY9GM2HzJFDJfCedN4QwYJy/A1wM4BlgOAI5kMnQQhEHh5YhGgqKoL1XcgXWQhF081hcc0C2EZjgIyfIHYPJiqgxggPglhgBAAcV4t8ikxAczRqaHJTHPIDBJ3ZCQecPiIaXqmQZi3BkYGen3NJFCzM+vqwRyAC6kIHebbtysdvc1NxpFmPew7vXKtJWYZpF0OhlhE0R4nFpY14emGmCb/WYribdtlFy8CjiRDeJjzsl80FOyN61cjl8RoMxRMTPwiEowImxHbwmIzgk2Ia6E9+sVTufvumvgV0/iQp5IQAGHSf++QE7xO44JsAAAAAElFTkSuQmCC"
                    },
                    "images": {
                      "fallback": {
                        "src": "/static/4201cbc38e4a1bbf8b47f652900a91b3/0ed2b/segnet-demo.png",
                        "srcSet": "/static/4201cbc38e4a1bbf8b47f652900a91b3/659f7/segnet-demo.png 175w,\n/static/4201cbc38e4a1bbf8b47f652900a91b3/7c282/segnet-demo.png 350w,\n/static/4201cbc38e4a1bbf8b47f652900a91b3/0ed2b/segnet-demo.png 700w",
                        "sizes": "(min-width: 700px) 700px, 100vw"
                      },
                      "sources": [
                        {
                          "srcSet": "/static/4201cbc38e4a1bbf8b47f652900a91b3/368a3/segnet-demo.avif 175w,\n/static/4201cbc38e4a1bbf8b47f652900a91b3/892ac/segnet-demo.avif 350w,\n/static/4201cbc38e4a1bbf8b47f652900a91b3/92717/segnet-demo.avif 700w",
                          "type": "image/avif",
                          "sizes": "(min-width: 700px) 700px, 100vw"
                        },
                        {
                          "srcSet": "/static/4201cbc38e4a1bbf8b47f652900a91b3/fb9f4/segnet-demo.webp 175w,\n/static/4201cbc38e4a1bbf8b47f652900a91b3/9fb2f/segnet-demo.webp 350w,\n/static/4201cbc38e4a1bbf8b47f652900a91b3/758f9/segnet-demo.webp 700w",
                          "type": "image/webp",
                          "sizes": "(min-width: 700px) 700px, 100vw"
                        }
                      ]
                    },
                    "width": 700,
                    "height": 418
                  }
                }
              },
              "tech": ["Semantic segmentation", "Pytorch", "Segnet"],
              "github": null,
              "external": "./Btech_Thesis.pdf",
              "cta": null
            },
            "html": "<p>Trained models on mitade20k dataset and finetuned models by class imbalance methods and Yolo-object detection method to remove false-positive intersections, which is very useful in autonomous driving, automated parking allotment system.</p>"
          }
        }
      ]
    }
  }
}
