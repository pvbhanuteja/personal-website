{
  "data": {
    "featured": {
      "edges": [
        {
          "node": {
            "frontmatter": {
              "title": "Repaper",
              "cover": null,
              "tech": ["Transformers", "LayoutLMv3", "Huggingface", "Pytorch"],
              "github": "https://github.com/pvbhanuteja/repaper",
              "external": null,
              "cta": null
            },
            "html": "<p>An open source python package to create an editable PDF form from a handwritten form. I used LayoutLMv3 model trained on a\nQuestion-Answer dataset to identify key-value pairs from the paper and easy-ocr to extract the bounding boxes and text\ninformation.</p>"
          }
        },
        {
          "node": {
            "frontmatter": {
              "title": "Voice Conversion Transformer",
              "cover": {
                "childImageSharp": {
                  "gatsbyImageData": {
                    "layout": "constrained",
                    "placeholder": {
                      "fallback": "data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAALCAYAAAB/Ca1DAAAACXBIWXMAABYlAAAWJQFJUiTwAAACBElEQVQoz5WSS2/TQBSF/eNhxYYdayqBUBFSd62gqDxEQTShQoQ0JCl17bRx/PaM3zP2hzwpRZVgwZGOZuZqdB/nXIu/oO+37Ax7+r6n6/ptzJydOQcOEEIQRRFKKawoipEiY+EK9j9dc3kVQ99tM94W+HMXIqMoyjsNJEmC67oEQYDVtMpU/H6R8uLtCtcThHHK0gk5OLY5nceUtWI8izn87DC/8GiampOzkONJSNP2SCnZeGuiKMQasudScGZnvB55pLJiuljx/ovN7tEVh2MfWSrT/VBwNHEIAp+9dw7PDm0SUbLxrjmd/mS19rGqqkZrxWgW8fzIwV2ngDZjt5WgkDFSitvxNhuP+WJB4LkEnkOeSxPXN6pY/zQFKKvasG1btO7RXW/Ev3IcHPsSPwjxPM9o5/sbkjjGqusapVpa1ZGXiqZp+R+cL5dMJhPDxXKJNVQpC8mHbwH3d6aMp9fotiTNcvwoQ6nOuKx0T5RINkFiGmhaTdN2iDSlaRqzMsM/SymN1ppENribAt3B2bnHm5HNw90ZOwc2ad7y5KXNg6dTPn51qQrJzv45j/aWrP0UIVLjtNEwDGNykfHqZM29x1NmFwFpkuCHCSKvidOcqm4oKkVRaW522bzLpsN1V8znP8iy7K4pg+CDjnpo8QZSZkRhYBZ3a5Ox7M6iD9MN4/7GL0IpQymk4p+yAAAAAElFTkSuQmCC"
                    },
                    "images": {
                      "fallback": {
                        "src": "/static/877a5e0a1a87fbf7a49a7e7886b5d9b8/e58c6/vc_demo.png",
                        "srcSet": "/static/877a5e0a1a87fbf7a49a7e7886b5d9b8/dfbac/vc_demo.png 175w,\n/static/877a5e0a1a87fbf7a49a7e7886b5d9b8/1a264/vc_demo.png 350w,\n/static/877a5e0a1a87fbf7a49a7e7886b5d9b8/e58c6/vc_demo.png 700w,\n/static/877a5e0a1a87fbf7a49a7e7886b5d9b8/dedab/vc_demo.png 1400w",
                        "sizes": "(min-width: 700px) 700px, 100vw"
                      },
                      "sources": [
                        {
                          "srcSet": "/static/877a5e0a1a87fbf7a49a7e7886b5d9b8/55e84/vc_demo.avif 175w,\n/static/877a5e0a1a87fbf7a49a7e7886b5d9b8/1b102/vc_demo.avif 350w,\n/static/877a5e0a1a87fbf7a49a7e7886b5d9b8/fe1b9/vc_demo.avif 700w,\n/static/877a5e0a1a87fbf7a49a7e7886b5d9b8/d3a2e/vc_demo.avif 1400w",
                          "type": "image/avif",
                          "sizes": "(min-width: 700px) 700px, 100vw"
                        },
                        {
                          "srcSet": "/static/877a5e0a1a87fbf7a49a7e7886b5d9b8/1ab83/vc_demo.webp 175w,\n/static/877a5e0a1a87fbf7a49a7e7886b5d9b8/a8c7f/vc_demo.webp 350w,\n/static/877a5e0a1a87fbf7a49a7e7886b5d9b8/8c88b/vc_demo.webp 700w,\n/static/877a5e0a1a87fbf7a49a7e7886b5d9b8/633d7/vc_demo.webp 1400w",
                          "type": "image/webp",
                          "sizes": "(min-width: 700px) 700px, 100vw"
                        }
                      ]
                    },
                    "width": 700,
                    "height": 374
                  }
                }
              },
              "tech": [
                "Voice conversion",
                "Attention",
                "Transformer",
                "PPG, BNF",
                "Speaker embeddings"
              ],
              "github": null,
              "external": null,
              "cta": "https://bit.ly/vc-bhanu"
            },
            "html": "<p>For Any to Any voice conversion transformer the linguistic features and voice identity of an utterance are seperated and used independently to achieve any combination on voice conversion. BNF and Speaker embeddings are inputs and mel-spectrogram is predicted. Speech quality syntesised is very clear with good voice conversion.</p>"
          }
        },
        {
          "node": {
            "frontmatter": {
              "title": "MixrNet",
              "cover": {
                "childImageSharp": {
                  "gatsbyImageData": {
                    "layout": "constrained",
                    "placeholder": {
                      "fallback": "data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAANCAYAAACpUE5eAAAACXBIWXMAABYlAAAWJQFJUiTwAAABl0lEQVQ4y5WTT0vcQBiH/USi30J6Ezx47amwlx5FPfgJpHizKtSDUAVFd0UPRUoRRLQge5Olf2hZDWbdZLObZJJMMvOUiVkRm7Trj7wM5J155n3nNzOmtWYYRlorfnRbXLa/8sv99vDffEW+mFQM+q8Yew7044CPJ3OsbU6zXp8jkOEjJ05iEqVRBqZUKbQE2Gf18zzLW7Ns1BcJYkGaZXnu6HibhaUaJ2cN4lSMBgxDwdb2B740L4iUQClFlCSkQOPTLlOvJnn9ZgbbsfP5Jl8KVEUL/YHP21qNyfEJdvb2zfE9bvb990/Or85pXjcJRThahTKJqTf2WVl5x8FhAyklWZYhU8lz/deUp076vk8URViWhW3b+ei6Lo7rICIxOtCEqShNUzBmDDcqzmp4bXRFlAJVsfB9ENDKMlSS4EURqVK0BgNkxZWpBOoH+7h0nNzh0yBgNQzzijcdB99U/xKgkWnb73aRQtDp9bjzPIQQ3Hse2UsrNDKLby0Lr9/npt3m3rbpdDp4vV4lrLrlp++2RPofpvwBXXnk2PPVhtwAAAAASUVORK5CYII="
                    },
                    "images": {
                      "fallback": {
                        "src": "/static/9d592d528ad639abe4c657e70f67467b/f4b24/mixrnet-demo.png",
                        "srcSet": "/static/9d592d528ad639abe4c657e70f67467b/1ac8f/mixrnet-demo.png 175w,\n/static/9d592d528ad639abe4c657e70f67467b/7d021/mixrnet-demo.png 350w,\n/static/9d592d528ad639abe4c657e70f67467b/f4b24/mixrnet-demo.png 700w",
                        "sizes": "(min-width: 700px) 700px, 100vw"
                      },
                      "sources": [
                        {
                          "srcSet": "/static/9d592d528ad639abe4c657e70f67467b/f0c3a/mixrnet-demo.avif 175w,\n/static/9d592d528ad639abe4c657e70f67467b/39b02/mixrnet-demo.avif 350w,\n/static/9d592d528ad639abe4c657e70f67467b/fc965/mixrnet-demo.avif 700w",
                          "type": "image/avif",
                          "sizes": "(min-width: 700px) 700px, 100vw"
                        },
                        {
                          "srcSet": "/static/9d592d528ad639abe4c657e70f67467b/f74b7/mixrnet-demo.webp 175w,\n/static/9d592d528ad639abe4c657e70f67467b/aa3e5/mixrnet-demo.webp 350w,\n/static/9d592d528ad639abe4c657e70f67467b/46e1e/mixrnet-demo.webp 700w",
                          "type": "image/webp",
                          "sizes": "(min-width: 700px) 700px, 100vw"
                        }
                      ]
                    },
                    "width": 700,
                    "height": 443.99999999999994
                  }
                }
              },
              "tech": ["Mixup", "Regularization", "Resnet", "Pytorch"],
              "github": "https://github.com/pvbhanuteja/mixrnet",
              "external": "https://arxiv.org/pdf/2111.11616.pdf",
              "cta": null
            },
            "html": "<p>Using mixup data augmentation technique as regularization and improving the ResNet50 architecture performance on image classification tasks. Achieved an error of 4.87% on CIFAR‑10 data‑set (Top 105 on CIFAR‑10 bench‑marking).</p>"
          }
        },
        {
          "node": {
            "frontmatter": {
              "title": "Cricket songs classification",
              "cover": null,
              "tech": [
                "AST - Audio Spectrogram Transformer",
                "Fine-tuning",
                "GPU Training",
                "Mel-spectrogram"
              ],
              "github": "https://github.com/pvbhanuteja/cricket-classification",
              "external": null,
              "cta": null
            },
            "html": "<p>Using AST model to classify cricket songs. Cricket songs are classified into 23 categories based on the species and their chirp. AST model finetuned on limited data and trained on 2 V100 GPUs.</p>"
          }
        },
        {
          "node": {
            "frontmatter": {
              "title": "Semantic Segmentation - Thesis",
              "cover": null,
              "tech": ["Semantic segmentation", "Pytorch", "Segnet"],
              "github": null,
              "external": "./Btech_Thesis.pdf",
              "cta": null
            },
            "html": "<p>Trained models on mitade20k dataset and finetuned models by class imbalance methods and Yolo-object detection method to remove false-positive intersections, which is very useful in autonomous driving, automated parking allotment system.</p>"
          }
        }
      ]
    }
  }
}
